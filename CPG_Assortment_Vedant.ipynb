{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a3988-8d3a-43a4-96b0-6aa5be71cb0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fosforml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/2e/3613fd0ccdbf3709dec86f87fe7624737a6f08bd1a813c88e65e7352dfde/fosforml-1.1.8-py3-none-any.whl (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting snowflake-ml-python==1.5.0; python_version <= \"3.9\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/72/c0fa5a9bc811a59a5a1c7113ff89676ed1629d7d6463db8c1a8c97a8b5f6/snowflake_ml_python-1.5.0-py3-none-any.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 9.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==1.3.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/89/dce01a35d354159dcc901e3c7e7eb3fe98de5cb3639c6cd39518d8830caa/scikit_learn-1.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9MB)\n",
      "\u001b[K     |████████████████████████████████| 10.9MB 29.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle==2.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/15/80/44286939ca215e88fa827b2aeb6fa3fd2b4a7af322485c7170d6f9fd96e0/cloudpickle-2.2.1-py3-none-any.whl\n",
      "Collecting cachetools<6,>=3.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/07/14f8ad37f2d12a5ce41206c21820d8cb6561b728e51fad4530dff0552a67/cachetools-5.5.0-py3-none-any.whl\n",
      "Collecting absl-py<2,>=0.15\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/87/de5c32fa1b1c6c3305d576e299801d8655c175ca9557019906247b994331/absl_py-1.4.0-py3-none-any.whl (126kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 104.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytimeparse<2,>=1.1.8\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/b4/afd75551a3b910abd1d922dbd45e49e5deeb4d47dc50209ce489ba9844dd/pytimeparse-1.1.8-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pyyaml<7,>=6.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (6.0.1)\n",
      "Collecting retrying<2,>=1.3.3\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/04/9e36f28be4c0532c0e9207ff9dc01fb13a2b0eb036476a213b0000837d0e/retrying-1.3.4-py3-none-any.whl\n",
      "Collecting sqlparse<1,>=0.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/a5/b2860373aa8de1e626b2bdfdd6df4355f0565b47e51f7d0c54fe70faf8fe/sqlparse-0.5.1-py3-none-any.whl (44kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 15.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources<7,>=6.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/6a/4604f9ae2fa62ef47b9de2fa5ad599589d28c9fd1d335f32759813dfa91e/importlib_resources-6.4.5-py3-none-any.whl\n",
      "Collecting anyio<4,>=3.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl (80kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 20.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting snowflake-connector-python[pandas]<4,>=3.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/78/7e3ce98acef501ab3509958ed8f95e31720e225b7f478b7d7e70ff731fc9/snowflake_connector_python-3.12.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5MB 109.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas<3,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (2.2.2)\n",
      "Collecting fsspec[http]<2024,>=2022.11\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/25/fab23259a52ece5670dcb8452e1af34b89e6135ecc17cd4b54b4b479eac6/fsspec-2023.12.2-py3-none-any.whl (168kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 103.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catboost<1.3,>=1.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/4a/8ada4fb635601943805cbeeb288f70982fefe48a7cef63352e318435aa98/catboost-1.2.7-cp39-cp39-manylinux2014_x86_64.whl (98.7MB)\n",
      "\u001b[K     |████████████████████████████████| 98.7MB 171kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting snowflake-snowpark-python!=1.12.0,<2,>=1.11.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/1e/c78cf80cc17dfe32bd89d8c5b7d86982571d58b4d8399bfecb8296e60bf1/snowflake_snowpark_python-1.22.1-py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 78.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xgboost<2,>=1.7.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/3a/c9c5d4d5c49b132ef15ac7b5ccf56ef1c82efe36cd19414771762e97c00e/xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3MB)\n",
      "\u001b[K     |████████████████████████████████| 200.3MB 89kB/s /s eta 0:00:01 | 164.0MB 153.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy<2,>=1.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f5/d0ad1a96f80962ba65e2ce1de6a1e59edecd1f0a7b55990ed208848012e0/scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6MB)\n",
      "\u001b[K     |████████████████████████████████| 38.6MB 79.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/61/bcd9b58e38ead6ad42b9ed00da33a3f862bc1d445e3d3164799c25550ac2/pyarrow-17.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9MB)\n",
      "\u001b[K     |████████████████████████████████| 39.9MB 40.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions<5,>=4.1.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (4.12.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2,>=1.23 in /opt/conda/lib/python3.9/site-packages (from snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (1.26.4)\n",
      "Collecting packaging<24,>=20.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 18.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs<2024,>=2022.11\n",
      "  Downloading https://files.pythonhosted.org/packages/5b/d6/b8a748b7d3fc7b0fd2ede1cf26a80281d65cc24d5d56b66c3a4c87e256e2/s3fs-2023.12.2-py3-none-any.whl\n",
      "Collecting joblib>=1.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl (301kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 109.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: six>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from retrying<2,>=1.3.3->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=3.1.0; python_version < \"3.10\" in /opt/conda/lib/python3.9/site-packages (from importlib-resources<7,>=6.1.1->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (3.19.2)\n",
      "Requirement already satisfied, skipping upgrade: sniffio>=1.1 in /opt/conda/lib/python3.9/site-packages (from anyio<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.8 in /opt/conda/lib/python3.9/site-packages (from anyio<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (3.7)\n",
      "Requirement already satisfied, skipping upgrade: exceptiongroup; python_version < \"3.11\" in /opt/conda/lib/python3.9/site-packages (from anyio<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (42.0.5)\n",
      "Collecting sortedcontainers>=2.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/32/46/9cb0e58b2deb7f82b84065f37f3bffeb12413f947f9388e4cac22c4621ce/sortedcontainers-2.4.0-py2.py3-none-any.whl\n",
      "Collecting asn1crypto<2.0.0,>0.24.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/7f/09065fd9e27da0eda08b4d6897f1c13535066174cc023af248fc2a8d5e5a/asn1crypto-1.5.1-py2.py3-none-any.whl (105kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 113.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (4.2.2)\n",
      "Collecting tomlkit\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/b6/a447b5e4ec71e13871be01ba81f5dfc9d0af7e473da256ff46bc0e24026f/tomlkit-0.13.2-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pyOpenSSL<25.0.0,>=16.2.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (24.0.0)\n",
      "Collecting filelock<4,>=3.5\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/f8/feced7779d755758a52d1f6635d990b8d98dc0a29fa568bbe0625f18fdf3/filelock-3.16.1-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (1.16.0)\n",
      "Collecting pyjwt<3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/79/84/0fdf9b18ba31d69877bd39c9cd6052b47f3761e9910c15de788e519f079f/PyJWT-2.9.0-py3-none-any.whl\n",
      "Collecting urllib3<2.0.0,>=1.21.1; python_version < \"3.10\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/cf/8435d5a7159e2a9c83a95896ed596f68cf798005fe107cc655b5c5c14704/urllib3-1.26.20-py2.py3-none-any.whl (144kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 97.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3.0.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (2.32.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (2024.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (2024.7.4)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (3.3.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas<3,>=1.0.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (2.9.0.post0)\n",
      "Requirement already satisfied, skipping upgrade: tzdata>=2022.7 in /opt/conda/lib/python3.9/site-packages (from pandas<3,>=1.0.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (2024.1)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/b3/8b20a25c896c6286e411be8fc6f3280bb9d7e9e4c9c90d0618a6c004ab2c/aiohttp-3.10.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 70.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting graphviz\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/be/d59db2d1d52697c6adc9eacaf50e8965b6345cc143f671e1ed068818d5cf/graphviz-0.20.3-py3-none-any.whl (47kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 13.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plotly\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/ae/580600f441f6fc05218bd6c9d5794f4aef072a7d9093b291f1c50a9db8bc/plotly-5.24.1-py3-none-any.whl (19.1MB)\n",
      "\u001b[K     |████████████████████████████████| 19.1MB 76.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/20/f56d5b88450593ccde3f283e338f3f976b2e479bddd9a147f14f66ee1ca7/matplotlib-3.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3MB)\n",
      "\u001b[K     |████████████████████████████████| 8.3MB 83.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools>=40.6.0 in /opt/conda/lib/python3.9/site-packages (from snowflake-snowpark-python!=1.12.0,<2,>=1.11.1->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (65.6.3)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /opt/conda/lib/python3.9/site-packages (from snowflake-snowpark-python!=1.12.0,<2,>=1.11.1->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (0.37.1)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/b8/a0fc9cf540d15f4962448c7ec097e99196b4ddd2a988d98b3d4236cbc89e/aiobotocore-2.15.1-py3-none-any.whl (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 16.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]<4,>=3.5.0->snowflake-ml-python==1.5.0; python_version <= \"3.9\"->fosforml) (2.21)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading https://files.pythonhosted.org/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl\n",
      "Collecting async-timeout<5.0,>=4.0; python_version < \"3.11\"\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade fosforml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057690e-6bfe-4893-b838-043f96a50860",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba8016-4659-4c2a-92fb-95b55a6ffe5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46685f5f-3937-4cd9-8c70-2152cd57b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install snowflake-ml-python\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5093cadf-ea3b-45c3-ba9a-7d542f411f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install ydata-profiling --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ae4ee-faec-48e6-a9ca-8f20c897407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fosforml\n",
    "from fosforml.model_manager.snowflakesession import get_session\n",
    "my_session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7ba14-43e5-4a4d-a662-f1a26b39a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985bf93-a607-4ac6-bc27-bc3d6da5d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_session.connection.database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a29801-fc98-4bb6-a475-cca6169cfe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_session.connection.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5df006-be98-40a6-8744-61fe8a6fb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"ASSORTMENT_PLANNING.CPG_BRONZE.SALES_CLEAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85aa19e-34df-4e7c-8968-3450060e4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = my_session.sql(\"select * from {}\".format(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57a10f-f63b-46e4-8d64-1f87ae2beb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e6882-8c92-416c-82d1-ac1e5a0ac944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sf_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540d696-044d-47c5-9dca-6ab172978258",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e44ad-40cb-4173-abfd-949fdb5dd235",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7fee48-4fdb-4d93-a65d-c053c2e24590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f131d-10fa-4215-a6d1-5b52313b58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c8bfd7-64e4-4382-b0ac-5dad5a2abaec",
   "metadata": {},
   "source": [
    "## Converting to datetime data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87487c4-635b-4bd7-ae5a-06b7807464c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TRANS_DATE'] = pd.to_datetime(df['TRANS_DATE'])\n",
    "df['START_DATE'] = pd.to_datetime(df['START_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9912ad-c2b5-4359-9e95-f6c3c8ea808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3a5ca-642f-47f2-b196-c822f343687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d5d09-8684-4dc6-a69e-444edf71c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34225015-add9-49a2-9268-359c8fa69da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361a820-0f90-45d7-8b9a-1bc41748da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique dates\n",
    "df['TRANS_DATE'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cc86c-2eeb-4b72-be06-3bf49a82d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features from date column\n",
    "df['YEAR'] = df['TRANS_DATE'].dt.year\n",
    "df['MONTH'] = df['TRANS_DATE'].dt.month\n",
    "df['DAY'] = df['TRANS_DATE'].dt.day\n",
    "df['DAY_OF_WEEK'] = df['TRANS_DATE'].dt.day_of_week + 1 # Monday is 1 and sunday is 7\n",
    "df['QUARTER'] = df['TRANS_DATE'].dt.quarter\n",
    "df['DAY_OF_YEAR'] = df['TRANS_DATE'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381c813-c0ea-4d17-801c-25b524bb9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set option to display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b62c6-f6ef-4ab3-8341-2d178e828185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ed5c0-5f1c-4df6-b735-d2f3213f86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from ydata_profiling import ProfileReport\n",
    "profile=ProfileReport(df,explorative=True)\n",
    "profile.to_file(\"autoeda_v1.html\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c81ea-f6f4-4980-a506-6efc2c7006c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup = df[df.duplicated()].sort_values(by=['TRANS_DATE','SALES_UNITS','OUTLET_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419541d6-b2a8-41bd-814e-4d8514ba977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265025cf-20dc-43e0-8e50-efd2b24d276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828e1d7-b76f-4d96-b672-7aa748af341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df_unique.sort_values(by='TRANS_DATE')\n",
    "df_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df70bb2-4ecd-433a-8e90-d50b11c1bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_dates = pd.date_range(start='2023-08-29', end='2024-08-27').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94d820-5741-441c-9962-4f763fcdc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates=set(df_all_dates) - set(df_unique['TRANS_DATE'])\n",
    "len(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05dac02-54d8-46aa-9423-f5a6f19fdc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the week number for each date\n",
    "df_check['WEEK_NUMBER'] = df_check['TRANS_DATE'].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fe874-8f89-4653-b105-7e452e7a3c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa0866-8123-41d3-9259-a962604a3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the year difference from the first date\n",
    "df_check['YEAR_DIFF'] = df_check['TRANS_DATE'].dt.year - df_check['TRANS_DATE'].dt.year.min()\n",
    "\n",
    "# Calculate the continuous week number\n",
    "df_check['CONTINUOUS_WEEK_NUMBER'] = df_check['WEEK_NUMBER'] + df_check['YEAR_DIFF'] * 52\n",
    "\n",
    "# Adjust for the first year weeks\n",
    "first_year_weeks = df_check[df_check['YEAR_DIFF'] == 0]['WEEK_NUMBER'].max()\n",
    "df_check['CONTINUOUS_WEEK_NUMBER'] = df_check.apply(\n",
    "    lambda row: row['CONTINUOUS_WEEK_NUMBER'] - (52 - first_year_weeks) if row['YEAR_DIFF'] > 0 else row['CONTINUOUS_WEEK_NUMBER'],\n",
    "    axis=1\n",
    ")\n",
    "df_check['CONTINUOUS_WEEK_NUMBER']=df_check['CONTINUOUS_WEEK_NUMBER']-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa184046-3423-4e84-bc7e-f097c7407504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot SALES_UNITS over CONTINUOUS_WEEK_NUMBER\n",
    "plt.figure(figsize=(10, 5))\n",
    "df_check.groupby('CONTINUOUS_WEEK_NUMBER')['SALES_UNITS'].sum().plot(kind='bar', color='orange')\n",
    "plt.title('SALES_UNITS over CONTINUOUS_WEEK_NUMBER')\n",
    "plt.xlabel('CONTINUOUS_WEEK_NUMBER')\n",
    "plt.ylabel('SALES_UNITS')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot SALES_UNITS over DAY_OF_WEEK\n",
    "plt.figure(figsize=(10, 5))\n",
    "df_check.groupby('DAY_OF_WEEK')['SALES_UNITS'].sum().plot(kind='bar', color='salmon')\n",
    "plt.title('SALES_UNITS over DAY_OF_WEEK')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('SALES_UNITS')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot SALES_UNITS over TRANSACTION DATE\n",
    "plt.figure(figsize=(10, 5))\n",
    "df_check.groupby('TRANS_DATE')['SALES_UNITS'].sum().plot(kind='line', color='lightgreen')\n",
    "plt.title('SALES_UNITS over DAY')\n",
    "plt.xlabel('TRANSACTION DATE')\n",
    "plt.ylabel('SALES_UNITS')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot SALES_UNITS over MONTH\n",
    "plt.figure(figsize=(10, 5))\n",
    "df_check.groupby('MONTH')['SALES_UNITS'].sum().plot(kind='bar', color='skyblue')\n",
    "plt.title('SALES_UNITS over MONTH')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('SALES_UNITS')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd69bc2-ecf7-470a-9027-2bdfd20bdd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_check.drop(['WEEK_NUMBER'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586f882-f50c-4726-897b-834a47054d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78d384d-b6e9-46b5-b93a-a89a2a799622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['OUTLET_CODE'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0861c9c-9cc1-427c-86e6-e207e46268c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In case cluster column exists, uncomment this\n",
    "#df_final = df_final.drop(columns=['CLUSTER'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a04bb-53bd-402b-a269-7351f1968f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional Features\n",
    "\n",
    "df_final['FREQUENCY'] = df_final.groupby(['OUTLET_CODE', 'PRODUCT_CODE']).cumcount() + 1\n",
    "\n",
    "df_final['PROFIT_PER_UNIT'] = (df_final['SALES_PTR_VALUE'] - df_final['SALES_VALUE']) / df_final['SALES_UNITS']\n",
    "\n",
    "df_final['DAYS_BETWEEN'] = df_final.groupby(['OUTLET_CODE', 'PRODUCT_CODE'])['TRANS_DATE'].diff().dt.days\n",
    "df_final['DAYS_BETWEEN'] = df_final['DAYS_BETWEEN'].fillna(0)\n",
    "\n",
    "df_final['UNIT_PTR'] = df_final['SALES_PTR_VALUE']/df_final['SALES_UNITS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8c447-b291-4126-88c3-fbf6c69a30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[(df_final['PRODUCT_CODE'] == 'PRD0147') & (df_final['OUTLET_CODE'] == 'OL160188')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419d31f-cc4e-43e0-bf85-c840a7ce7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# List of columns to encode\n",
    "columns_to_encode = ['BRAND', 'PRODUCT_CODE', 'SUBCATEGORY', 'CITY','STATE', 'COUNTY']\n",
    "\n",
    "# Apply label encoding to each column\n",
    "for column in columns_to_encode:\n",
    "    df_final[column + '_encoded'] = label_encoder.fit_transform(df_final[column])\n",
    "\n",
    "df_final = pd.get_dummies(df_final, columns=['DISTRIBUTOR_CODE', 'CATEGORY'])\n",
    "\n",
    "# Convert all column names to uppercase and replace spaces with underscores\n",
    "df_final.columns = df_final.columns.str.upper().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a8ce0-dae5-442d-9e73-f167b4e001ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named df_final\n",
    "# Convert all column names to uppercase and replace spaces with underscores\n",
    "df_final.columns = df_final.columns.str.upper().str.replace(' ', '_')\n",
    "\n",
    "# List of one-hot encoded category columns\n",
    "category_columns = [\n",
    "    'CATEGORY_DENTAL', 'CATEGORY_HAIR_CARE', 'CATEGORY_KIDS_CARE',\n",
    "    'CATEGORY_LOTION', 'CATEGORY_PERFUME_AND_DEODRANTS', 'CATEGORY_SOAP', 'CATEGORY_WIPES'\n",
    "]\n",
    "\n",
    "# Aggregate data by OUTLET_CODE, including one-hot encoded category columns\n",
    "aggregation_dict = {\n",
    "    'SALES_UNITS': 'mean',\n",
    "    'PROFIT_PER_UNIT': 'mean',\n",
    "    'FREQUENCY': 'count'\n",
    "}\n",
    "aggregation_dict.update({col: 'max' for col in category_columns})\n",
    "\n",
    "aggregated_df = df_final.groupby('OUTLET_CODE').agg(aggregation_dict).reset_index()\n",
    "\n",
    "# Select features for clustering\n",
    "features = ['SALES_UNITS', 'PROFIT_PER_UNIT', 'FREQUENCY'] + category_columns\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['SALES_UNITS', 'PROFIT_PER_UNIT', 'FREQUENCY']),\n",
    "        # No need to preprocess category columns as they are already one-hot encoded\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep the one-hot encoded columns as they are\n",
    ")\n",
    "\n",
    "# Preprocess the data\n",
    "df_preprocessed = preprocessor.fit_transform(aggregated_df[features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8967c47-f669-4643-be56-0e6a4d10696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate silhouette scores for different numbers of clusters\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)  # Silhouette score is not defined for k=1\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(df_preprocessed)\n",
    "    silhouette_avg = silhouette_score(df_preprocessed, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0852577-401b-459b-bca8-d4df2e004e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering with the chosen number of clusters \n",
    "optimal_k = k_range[silhouette_scores.index(max(silhouette_scores))]  # Choose the k with the highest silhouette score\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "aggregated_df['CLUSTER'] = kmeans.fit_predict(df_preprocessed)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(aggregated_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208e666-be37-438f-b180-41374b3c34a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example visualization Clusters of SALES_UNITS VS SALES_VALUE\n",
    "sns.scatterplot(x='SALES_UNITS', y='FREQUENCY', hue='CLUSTER', data=aggregated_df)\n",
    "plt.title('Clusters of OUTLET_CODE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b567da5-bc29-4ba1-bf7b-b67cfe169bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_with_clusters = df_final.merge(aggregated_df[['OUTLET_CODE', 'CLUSTER']], on='OUTLET_CODE', how='left')\n",
    "df_final_with_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19495268-6236-4646-84c4-8c33a9405c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featureset = df_final_with_clusters.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d476ac4-979f-42ad-a858-f5cfecc626fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_featureset.drop(['OC_CODE','OUTLET_CODE','PRODUCT_CODE','STREET','CITY','STATE','COUNTY','PRODUCT_CODE','SUBCATEGORY','BRAND'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae4c97-dcf7-41b6-9418-840568c3b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featureset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a284d-1a26-4c87-b534-5b7402f1f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# Separate features and target\n",
    "X = df_featureset.drop('SALES_UNITS', axis=1)\n",
    "y = df_featureset['SALES_UNITS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218b596-4597-401f-a425-3271845ae096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming df_featureset is your DataFrame\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    'YEAR', \n",
    "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DAY_OF_YEAR', 'FREQUENCY',\n",
    "    'BRAND_ENCODED', 'PRODUCT_CODE_ENCODED', 'SUBCATEGORY_ENCODED', 'CITY_ENCODED',\n",
    "    'STATE_ENCODED',\n",
    "    # 'COUNTY_ENCODED', \n",
    "    # 'DISTRIBUTOR_CODE_DB0110', 'DISTRIBUTOR_CODE_DB0209',\n",
    "    # 'DISTRIBUTOR_CODE_DB0652', 'DISTRIBUTOR_CODE_DB0655', 'DISTRIBUTOR_CODE_DB0706',\n",
    "    # # 'CATEGORY_DENTAL', 'CATEGORY_HAIR_CARE', 'CATEGORY_KIDS_CARE', 'CATEGORY_LOTION',\n",
    "    # 'CATEGORY_PERFUME_AND_DEODRANTS', 'CATEGORY_SOAP', 'CATEGORY_WIPES', \n",
    "    'CLUSTER','UNIT_PTR'\n",
    "]\n",
    "target = 'SALES_UNITS'\n",
    "\n",
    "# Split the data into training and testing sets based on MNTH_CODE\n",
    "train_data = df_featureset[df_featureset['MNTH_CODE'] != 202408]\n",
    "test_data = df_featureset[df_featureset['MNTH_CODE'] == 202408]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "# Display feature importances\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)\n",
    "print(\"Feature Importances:\\n\", feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89718f-c3e7-4362-a656-0d2e841a885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to compare actual and predicted values\n",
    "comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "\n",
    "comparison_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb78a56-e7ec-4574-9f3c-71e9c1e560c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Assuming y_test and y_pred are your actual and predicted values respectively\n",
    "\n",
    "# Define a threshold to convert continuous predictions to binary labels\n",
    "threshold = 10  # Example threshold\n",
    "\n",
    "# Convert to binary labels based on the threshold\n",
    "y_test_binary = (y_test >= threshold).astype(int)\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "# Compute precision and recall\n",
    "precision = precision_score(y_test_binary, y_pred_binary)\n",
    "recall = recall_score(y_test_binary, y_pred_binary)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14659bd3-022d-47e4-9920-e0a4e6bff352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming X_test is your test features DataFrame\n",
    "# # And comparison_df is your DataFrame containing actual and predicted values\n",
    "\n",
    "# # Ensure the indices match\n",
    "# comparison_df.index = X_test.index\n",
    "\n",
    "# # Concatenate X_test with comparison_df\n",
    "# merged_df = pd.concat([X_test, comparison_df], axis=1)\n",
    "\n",
    "# # Display the first few rows to verify\n",
    "# merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8e2fb-bef5-44a6-b330-a9eb1362d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# # Assuming y_test and y_pred are your actual and predicted values respectively\n",
    "# # And comparison_df is your DataFrame containing actual and predicted values\n",
    "\n",
    "# # Ensure y_test and y_pred are integers representing class labels\n",
    "# y_test_multiclass = y_test.astype(int)\n",
    "# y_pred_multiclass = y_pred.astype(int)\n",
    "\n",
    "# # Calculate precision and recall for multiclass\n",
    "# precision = precision_score(y_test_multiclass, y_pred_multiclass, average='macro')\n",
    "# recall = recall_score(y_test_multiclass, y_pred_multiclass, average='macro')\n",
    "\n",
    "# # Add precision and recall as columns to comparison_df\n",
    "# comparison_df['PRECISION'] = precision\n",
    "# comparison_df['RECALL'] = recall\n",
    "\n",
    "# # Display the first few rows to verify\n",
    "# comparison_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c116176-57a2-4ebb-8b29-80ed2eabfe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming the one-hot encoded columns are in merged_df\n",
    "# # Create a mapping for distributor codes and categories\n",
    "# distributor_columns = ['DISTRIBUTOR_CODE_DB0110', 'DISTRIBUTOR_CODE_DB0209', 'DISTRIBUTOR_CODE_DB0652', 'DISTRIBUTOR_CODE_DB0655', 'DISTRIBUTOR_CODE_DB0706']\n",
    "# category_columns = ['CATEGORY_DENTAL', 'CATEGORY_HAIR_CARE', 'CATEGORY_KIDS_CARE', 'CATEGORY_LOTION', 'CATEGORY_PERFUME_AND_DEODRANTS', 'CATEGORY_SOAP', 'CATEGORY_WIPES']\n",
    "\n",
    "# # Decode distributor codes\n",
    "# merged_df['DISTRIBUTOR_CODE'] = merged_df[distributor_columns].idxmax(axis=1).str.replace('DISTRIBUTOR_CODE_', '')\n",
    "\n",
    "# # Decode categories\n",
    "# merged_df['CATEGORY'] = merged_df[category_columns].idxmax(axis=1).str.replace('CATEGORY_', '').str.replace('_', ' ')\n",
    "\n",
    "# # Drop the one-hot encoded columns if no longer needed\n",
    "# merged_df.drop(columns=distributor_columns + category_columns, inplace=True)\n",
    "\n",
    "# # Display the first few rows to verify\n",
    "# merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b5c03-fc15-4bb2-8a72-873258850f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Assuming df_final is your DataFrame before encoding\n",
    "# # Initialize the label encoder\n",
    "# label_encoders = {}\n",
    "\n",
    "# # List of columns to encode\n",
    "# columns_to_encode = ['BRAND', 'PRODUCT_CODE', 'SUBCATEGORY', 'CITY', 'STATE', 'COUNTY']\n",
    "\n",
    "# # Fit the label encoders with the original data\n",
    "# for column in columns_to_encode:\n",
    "#     le = LabelEncoder()\n",
    "#     le.fit(df_final[column])\n",
    "#     label_encoders[column] = le\n",
    "\n",
    "# # Inverse transform the encoded columns in merged_df\n",
    "# merged_df['BRAND'] = label_encoders['BRAND'].inverse_transform(merged_df['BRAND_ENCODED'])\n",
    "# merged_df['PRODUCT_CODE'] = label_encoders['PRODUCT_CODE'].inverse_transform(merged_df['PRODUCT_CODE_ENCODED'])\n",
    "# merged_df['SUBCATEGORY'] = label_encoders['SUBCATEGORY'].inverse_transform(merged_df['SUBCATEGORY_ENCODED'])\n",
    "# merged_df['CITY'] = label_encoders['CITY'].inverse_transform(merged_df['CITY_ENCODED'])\n",
    "# merged_df['STATE'] = label_encoders['STATE'].inverse_transform(merged_df['STATE_ENCODED'])\n",
    "# merged_df['COUNTY'] = label_encoders['COUNTY'].inverse_transform(merged_df['COUNTY_ENCODED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddece24e-f3c9-45fa-a8f0-775b7b6a7761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of columns to drop\n",
    "# columns_to_drop = ['BRAND_ENCODED', 'PRODUCT_CODE_ENCODED', 'SUBCATEGORY_ENCODED', 'CITY_ENCODED', 'STATE_ENCODED', 'COUNTY_ENCODED']\n",
    "\n",
    "# # Drop the specified columns\n",
    "# merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# merged_df = merged_df.merge(df_final_with_clusters[['CITY', 'STATE', 'COUNTY', 'OUTLET_CODE']], on=['CITY', 'STATE', 'COUNTY'], how='left')\n",
    "\n",
    "# merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72b12d-73ca-474f-9d33-d23969761152",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a Future dataset for all possible outlet code & product code combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d4b20-c34a-4ad4-a39d-03e50da187ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Extract unique values of OUTLET_CODE and PRODUCT_CODE\n",
    "outlet_codes = df['OUTLET_CODE'].unique()\n",
    "product_codes = df['PRODUCT_CODE'].unique()\n",
    "\n",
    "# Generate all possible combinations of OUTLET_CODE and PRODUCT_CODE\n",
    "combinations = list(itertools.product(outlet_codes, product_codes))\n",
    "\n",
    "# Create a DataFrame from the combinations\n",
    "future_df = pd.DataFrame(combinations, columns=['OUTLET_CODE', 'PRODUCT_CODE'])\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(future_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94062b3-f7b9-4412-8cef-c2f04c808530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by OUTLET_CODE and count unique DISTRIBUTOR_CODE\n",
    "distributor_counts = df.groupby('OUTLET_CODE')['DISTRIBUTOR_CODE'].nunique().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "distributor_counts.rename(columns={'DISTRIBUTOR_CODE': 'UNIQUE_DISTRIBUTOR_COUNT'}, inplace=True)\n",
    "\n",
    "# Determine if each OUTLET_CODE has multiple or a single DISTRIBUTOR_CODE\n",
    "distributor_counts['DISTRIBUTOR_TYPE'] = distributor_counts['UNIQUE_DISTRIBUTOR_COUNT'].apply(lambda x: 'Multiple' if x > 1 else 'Single')\n",
    "distributor_counts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ae584-0811-4bcc-8e02-fa069fb1251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distributor_counts['UNIQUE_DISTRIBUTOR_COUNT'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2f2d1-36b7-4945-a6f1-b7f543c02f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112f1dc-8dff-467b-ba5f-5a2288210523",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df = future_df.merge(df_final_with_clusters[['OUTLET_CODE', 'CLUSTER']], on='OUTLET_CODE', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ab86e-4007-4f5f-8f7c-2b55cb1af224",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df = future_df.drop(columns=['OUTLET_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e76e8-1d82-4dc9-9e5f-4458b31fa22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845d79e-2a63-4697-a146-21a2ca8d0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df= future_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903b5b4-9469-412c-9902-1e38ee243442",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_cluster_df = df_final_with_clusters[['PRODUCT_CODE', 'CLUSTER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3c0be-0a80-4087-a859-0293b0ccff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_cluster_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2712ade-e68d-469a-9bfc-051e36348865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming df and future_df are your DataFrames\n",
    "\n",
    "# # Merge the DISTRIBUTOR_CODE column from df with future_df\n",
    "# future_df = future_df.merge(df[['OUTLET_CODE', 'DISTRIBUTOR_CODE']], on='OUTLET_CODE', how='left')\n",
    "# future_df = future_df.merge(df_final[['PRODUCT_CODE', 'UNIT_PTR']], on='PRODUCT_CODE', how='left')\n",
    "\n",
    "# # Display the first few rows to verify\n",
    "# print(future_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399ca22-658b-4cf9-9404-5d2f660531da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge the CLUSTER column from df with future_df\n",
    "# future_df = future_df.merge(df_final_with_clusters[['OUTLET_CODE','CLUSTER']], on='OUTLET_CODE', how='left')\n",
    "\n",
    "# # Display the first few rows to verify\n",
    "# print(future_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
